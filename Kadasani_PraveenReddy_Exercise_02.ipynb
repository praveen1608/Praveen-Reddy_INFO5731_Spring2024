{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/praveen1608/Praveen-Reddy_INFO5731_Spring2024/blob/main/Kadasani_PraveenReddy_Exercise_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "\"\"\"\n",
        "How does the performance of a product sales depend on customer reviews?\n",
        "\n",
        "Here we are using web scrapping techniques to collect product reviews from the amazon.\n",
        "\n",
        "we are conducting a complete analysis, so we should collect a good amount of data at least 1000 reviews for a product to analyse the sentiment of customers i.e overall rating of the product.\n",
        "\n",
        "We need to first get the URL link for the product we working on, by using the code we should scrape the product reviews to the extent we require. Now save the scraped data in a structured format like CSV or JSON.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "cikVKDXdTbzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_amazon_reviews(product_url, max_samples):\n",
        "    reviews_data = []\n",
        "    page_number = 1\n",
        "\n",
        "    while len(reviews_data) < max_samples:\n",
        "        # Construct URL for the current page\n",
        "        url = f\"{product_url}&pageNumber={page_number}\"\n",
        "\n",
        "        # Send HTTP GET request to product URL\n",
        "        response = requests.get(url)\n",
        "\n",
        "        # Parse HTML content using BeautifulSoup\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract review elements\n",
        "        review_elements = soup.find_all('div', class_='a-section review aok-relative')\n",
        "\n",
        "        # Extract review text and star ratings\n",
        "        for review in review_elements:\n",
        "            review_text = review.find('span', class_='review-text').text.strip()\n",
        "            star_rating = float(review.find('span', class_='a-icon-alt').text.split()[0])  # Extract star rating from alt text\n",
        "            reviews_data.append({'Review Text': review_text, 'Star Rating': star_rating})\n",
        "\n",
        "            # Check if we have collected enough samples\n",
        "            if len(reviews_data) >= max_samples:\n",
        "                break\n",
        "\n",
        "        # Move to the next page\n",
        "        page_number += 1\n",
        "\n",
        "    return reviews_data\n",
        "\n",
        "def main():\n",
        "    # Product URL to scrape reviews from\n",
        "    product_url = 'https://www.amazon.com/2022-Apple-iPad-10-9-inch-Wi-Fi/dp/B09V3JJT5D/ref=sr_1_1?crid=U18Z0W66Y3D2&keywords=ipad+air+5th+generation&qid=1708058290&sprefix=ipad+air+5th+generation+%2Caps%2C325&sr=8-1-spons&ufe=app_do%3Aamzn1.fos.c3015c4a-46bb-44b9-81a4-dc28e6d374b3&sp_csd=d2lkZ2V0TmFtZT1zcF9hdGY&psc=1'\n",
        "\n",
        "    # Number of samples to collect\n",
        "    max_samples = 1000\n",
        "\n",
        "    # Scrape product reviews from Amazon\n",
        "    reviews_data = scrape_amazon_reviews(product_url, max_samples)\n",
        "\n",
        "    # Convert reviews data to DataFrame\n",
        "    df = pd.DataFrame(reviews_data)\n",
        "\n",
        "    # Save data to CSV file\n",
        "    df.to_csv('amazon_reviews.csv', index=False)\n",
        "    print(\"Data saved successfully.\")\n",
        "\n",
        "    print(df)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "923767f6-1460-4899-e471-56d9d13e92a5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved successfully.\n",
            "                                           Review Text  Star Rating\n",
            "0    The iPad Air 5th Generation has been a perfect...          5.0\n",
            "1    I bought ipad air to carry in small bags when ...          5.0\n",
            "2    Started using an IPad back when the IPad 2 was...          5.0\n",
            "3    It's been years I've used an apple device with...          5.0\n",
            "4    I received this iPad Air as my Christmas gift ...          5.0\n",
            "..                                                 ...          ...\n",
            "995  The M1 chip in this thing rips. Was torn betwe...          5.0\n",
            "996  The iPad Air 5th Generation has been a perfect...          5.0\n",
            "997  I bought ipad air to carry in small bags when ...          5.0\n",
            "998  Started using an IPad back when the IPad 2 was...          5.0\n",
            "999  It's been years I've used an apple device with...          5.0\n",
            "\n",
            "[1000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "YaGLbSHHB8Ej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a102c90-c3a9-4876-ac8c-60f945e24e4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=0\n",
            "Articles collected so far: 10\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=10\n",
            "Articles collected so far: 20\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=20\n",
            "Articles collected so far: 30\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=30\n",
            "Articles collected so far: 40\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=40\n",
            "Articles collected so far: 50\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=50\n",
            "Articles collected so far: 60\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=60\n",
            "Articles collected so far: 70\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=70\n",
            "Articles collected so far: 80\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=80\n",
            "Articles collected so far: 90\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=90\n",
            "Articles collected so far: 100\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=100\n",
            "Articles collected so far: 110\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=110\n",
            "Articles collected so far: 120\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=120\n",
            "Articles collected so far: 130\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=130\n",
            "Articles collected so far: 140\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=140\n",
            "Articles collected so far: 150\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=150\n",
            "Articles collected so far: 160\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=160\n",
            "Articles collected so far: 170\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=170\n",
            "Articles collected so far: 180\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=180\n",
            "Articles collected so far: 190\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=190\n",
            "Articles collected so far: 200\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=200\n",
            "Articles collected so far: 210\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=210\n",
            "Articles collected so far: 220\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=220\n",
            "Articles collected so far: 230\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=230\n",
            "Articles collected so far: 240\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=240\n",
            "Articles collected so far: 250\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=250\n",
            "Articles collected so far: 260\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=260\n",
            "Articles collected so far: 270\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=270\n",
            "Articles collected so far: 280\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=280\n",
            "Articles collected so far: 290\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=290\n",
            "Articles collected so far: 300\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=300\n",
            "Articles collected so far: 310\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=310\n",
            "Articles collected so far: 320\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=320\n",
            "Articles collected so far: 330\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=330\n",
            "Articles collected so far: 340\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=340\n",
            "Articles collected so far: 350\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=350\n",
            "Articles collected so far: 360\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=360\n",
            "Articles collected so far: 370\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=370\n",
            "Articles collected so far: 380\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=380\n",
            "Articles collected so far: 390\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=390\n",
            "Articles collected so far: 400\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=400\n",
            "Articles collected so far: 410\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=410\n",
            "Articles collected so far: 420\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=420\n",
            "Articles collected so far: 430\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=430\n",
            "Articles collected so far: 440\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=440\n",
            "Articles collected so far: 450\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=450\n",
            "Articles collected so far: 460\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=460\n",
            "Articles collected so far: 470\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=470\n",
            "Articles collected so far: 480\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=480\n",
            "Articles collected so far: 490\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=490\n",
            "Articles collected so far: 500\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=500\n",
            "Articles collected so far: 510\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=510\n",
            "Articles collected so far: 520\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=520\n",
            "Articles collected so far: 530\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=530\n",
            "Articles collected so far: 540\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=540\n",
            "Articles collected so far: 550\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=550\n",
            "Articles collected so far: 560\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=560\n",
            "Articles collected so far: 570\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=570\n",
            "Articles collected so far: 580\n",
            "Scraping URL: https://scholar.google.com/scholar?q=XYZ&hl=en&start=580\n",
            "Data saved successfully.\n",
            "                                                 Title  \\\n",
            "0                             The XYZ states revisited   \n",
            "1    [BOOK][B] The ABC of XYZ: Understanding the gl...   \n",
            "2    The quantum method of the inverse problem and ...   \n",
            "3    The XYZ states: experimental and theoretical s...   \n",
            "4            Augmentation gluteoplasty: the XYZ method   \n",
            "..                                                 ...   \n",
            "575  [PDF][PDF] … dos materiais de enfermagem segun...   \n",
            "576                    [PDF][PDF] XYZ 系统的目的, 意义, 作用与应用   \n",
            "577  Penerapan teknologi qr code berbasis web pada ...   \n",
            "578  [PDF][PDF] Pengukuran Kinerja Operasional Mela...   \n",
            "579  [PDF][PDF] Analisis Performa Cooling Tower LCT...   \n",
            "\n",
            "                                               Authors  \\\n",
            "0    CZ Yuan - International Journal of Modern Phys...   \n",
            "1    M McCrindle, E Wolfinger - 2009 - books.google...   \n",
            "2    LA Takhtadzhan, LD Faddeev - Russian Mathemati...   \n",
            "3    N Brambilla, S Eidelman, C Hanhart, A Nefediev...   \n",
            "4    R Gonzalez - Aesthetic plastic surgery, 2004 -...   \n",
            "..                                                 ...   \n",
            "575  KGL Mendes, V Castilho - Rev Inst Ciênc Saúde,...   \n",
            "576                      唐稚松 - 软件学报, 1999 - jos.org.cn   \n",
            "577  JR Luih, CA Haryani, AE Widjaja - Technomedia ...   \n",
            "578  NA Malik, M Hamsal - Jurnal Bisnis dan Kewirau...   \n",
            "579  Y Handoyo - Jurnal Ilmiah Teknik Mesin, 2015 -...   \n",
            "\n",
            "                                                 Venue  \\\n",
            "0    CZ Yuan - International Journal of Modern Phys...   \n",
            "1    M McCrindle, E Wolfinger - 2009 - books.google...   \n",
            "2    LA Takhtadzhan, LD Faddeev - Russian Mathemati...   \n",
            "3    N Brambilla, S Eidelman, C Hanhart, A Nefediev...   \n",
            "4    R Gonzalez - Aesthetic plastic surgery, 2004 -...   \n",
            "..                                                 ...   \n",
            "575  KGL Mendes, V Castilho - Rev Inst Ciênc Saúde,...   \n",
            "576                      唐稚松 - 软件学报, 1999 - jos.org.cn   \n",
            "577  JR Luih, CA Haryani, AE Widjaja - Technomedia ...   \n",
            "578  NA Malik, M Hamsal - Jurnal Bisnis dan Kewirau...   \n",
            "579  Y Handoyo - Jurnal Ilmiah Teknik Mesin, 2015 -...   \n",
            "\n",
            "                                                  Year  \\\n",
            "0    CZ Yuan - International Journal of Modern Phys...   \n",
            "1    M McCrindle, E Wolfinger - 2009 - books.google...   \n",
            "2    LA Takhtadzhan, LD Faddeev - Russian Mathemati...   \n",
            "3    N Brambilla, S Eidelman, C Hanhart, A Nefediev...   \n",
            "4    R Gonzalez - Aesthetic plastic surgery, 2004 -...   \n",
            "..                                                 ...   \n",
            "575  KGL Mendes, V Castilho - Rev Inst Ciênc Saúde,...   \n",
            "576                      唐稚松 - 软件学报, 1999 - jos.org.cn   \n",
            "577  JR Luih, CA Haryani, AE Widjaja - Technomedia ...   \n",
            "578  NA Malik, M Hamsal - Jurnal Bisnis dan Kewirau...   \n",
            "579  Y Handoyo - Jurnal Ilmiah Teknik Mesin, 2015 -...   \n",
            "\n",
            "                                              Abstract  \n",
            "0    The BESIII and the LHCb became the leading exp...  \n",
            "1    Based on a more than a decade of research, thi...  \n",
            "2    … of the ground state of the XYZ model. In the...  \n",
            "3    The quark model was formulated in 1964 to clas...  \n",
            "4    Gluteal implants offer a good way not only to ...  \n",
            "..                                                 ...  \n",
            "575  … A Classificação XYZ é uma ferramenta importa...  \n",
            "576  … XYZ系统的态度虽已发生彻底变化.但这些都只限于到过我们研究所参观了 XYZ系统演示的少...  \n",
            "577  … XYZ merupakan perusahaan yang ingin menerapk...  \n",
            "578  This study describes the implementation of Tot...  \n",
            "579  Tujuan penelitian adalah untuk mengetahui perf...  \n",
            "\n",
            "[580 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "# write your answer here\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_google_scholar(keyword, max_articles):\n",
        "    base_url = \"https://scholar.google.com/scholar\"\n",
        "    params = {'q': keyword, 'hl': 'en', 'start': 0}\n",
        "    articles_data = []\n",
        "    articles_collected = 0\n",
        "\n",
        "    try:\n",
        "        while articles_collected < max_articles:\n",
        "            response = requests.get(base_url, params=params)\n",
        "            print(\"Scraping URL:\", response.url)  # Print the URL being scraped\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            articles = soup.find_all('div', class_='gs_r gs_or gs_scl')\n",
        "\n",
        "            if not articles:\n",
        "                break\n",
        "\n",
        "            for article in articles:\n",
        "                title = article.find('h3', class_='gs_rt').text.strip()\n",
        "                authors = article.find('div', class_='gs_a').text.strip()\n",
        "                venue = article.find('div', class_='gs_a').text.strip()\n",
        "                year = article.find('div', class_='gs_a').text.strip()\n",
        "                abstract = article.find('div', class_='gs_rs').text.strip()\n",
        "\n",
        "                articles_data.append({\n",
        "                    'Title': title,\n",
        "                    'Authors': authors,\n",
        "                    'Venue': venue,\n",
        "                    'Year': year,\n",
        "                    'Abstract': abstract\n",
        "                })\n",
        "\n",
        "                articles_collected += 1\n",
        "                if articles_collected >= max_articles:\n",
        "                    break\n",
        "\n",
        "            # Update params for next page\n",
        "            params['start'] += 10\n",
        "\n",
        "            # Print the length of articles_data after scraping each page\n",
        "            print(f\"Articles collected so far: {len(articles_data)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred during scraping:\", e)\n",
        "\n",
        "    return articles_data\n",
        "\n",
        "def main():\n",
        "    keyword = \"XYZ\"\n",
        "    max_articles = 1000\n",
        "\n",
        "    articles_data = scrape_google_scholar(keyword, max_articles)\n",
        "\n",
        "    # Convert data to DataFrame\n",
        "    df = pd.DataFrame(articles_data)\n",
        "\n",
        "    # Save data to CSV file\n",
        "    df.to_csv('articles_data.csv', index=False)\n",
        "    print(\"Data saved successfully.\")\n",
        "\n",
        "    # Print the collected data\n",
        "    print(df)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtKskTzbCLaU"
      },
      "outputs": [],
      "source": [
        "# write your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Write your response here.\n",
        "\n",
        "Its feels very nice to learn new things, I never did web scraping before, this feels good.\n",
        "It is like building our own things with our products rather than depending on others for data.\n",
        "I learnt about new libraries and how to use them effectively in web scraping rather than writing the long code. These libraries will save the time they were really very useful.\n",
        "\n",
        "I tried scraping with the twitter using API, where I faced the issues like \"you need more access or there is only limited access to you\".\n",
        "I think its restricted, twitter is not allowing us to do that now. So, I have gone with product reviews.\n",
        "\n",
        "And in question number 4 I want to try a differnt tool, so I worked on \"ParseHub\". At first I have faced a lot of problems on how to use it. It was very confusing.\n",
        "Later after several tries I got succeeded.\n",
        "'''"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}