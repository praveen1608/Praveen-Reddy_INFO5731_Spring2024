{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/praveen1608/Praveen-Reddy_INFO5731_Spring2024/blob/main/Kadasani_PraveenReddy_Exercise_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "dabe610e-d841-4241-c524-9a2e96b3b873"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nClassifing the sentiment analysis on customer reviews of a product.\\n\\nfeatures used-\\n\\n(1)Bag-of-Words (BoW):\\nBoW represents each document as a vector where every element is corresponding to the frequency of the word in that document.\\nThe Presence and frequency of the words can be captured by using this feature.\\n\\n(2)TF-IDF (Term Frequency-Inverse Document Frequency):\\nThis feature is much similar to BoW, but it calculates the importance of words by how many times they are repeating  in the entire corpus.\\nThis feature helps in calculating the uniqueness of the words in the document.\\n\\n(3)Word Embeddings:\\nIt represents words as dense vectors in a continuous vector space. This helps is capturing the semantic relationships between the words\\nwhich is used in understanding the meaning of the text.\\n\\n(4)N-grams:\\nThis feature instead of considering single words, it considers a sequence of words. Which helps in understanding the phrases and expressions \\nby capturing the contextual information.\\n\\n(5)Part-of-Speech(POS):\\nPOS tags represents grammar category of words in a sentence. They capture syntsctic information, revelant to sentiment analysis.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Classifing the sentiment analysis on customer reviews of a product.\n",
        "\n",
        "features used-\n",
        "\n",
        "(1)Bag-of-Words (BoW):\n",
        "BoW represents each document as a vector where every element is corresponding to the frequency of the word in that document.\n",
        "The Presence and frequency of the words can be captured by using this feature.\n",
        "\n",
        "(2)TF-IDF (Term Frequency-Inverse Document Frequency):\n",
        "This feature is much similar to BoW, but it calculates the importance of words by how many times they are repeating  in the entire corpus.\n",
        "This feature helps in calculating the uniqueness of the words in the document.\n",
        "\n",
        "(3)Word Embeddings:\n",
        "It represents words as dense vectors in a continuous vector space. This helps is capturing the semantic relationships between the words\n",
        "which is used in understanding the meaning of the text.\n",
        "\n",
        "(4)N-grams:\n",
        "This feature instead of considering single words, it considers a sequence of words. Which helps in understanding the phrases and expressions\n",
        "by capturing the contextual information.\n",
        "\n",
        "(5)Part-of-Speech(POS):\n",
        "POS tags represents grammar category of words in a sentence. They capture syntsctic information, revelant to sentiment analysis.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c640f73-60f4-45d9-fa9e-55f7234ecbb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bag-of-Words (BoW) features:\n",
            "['about' 'amazing' 'and' 'consistently' 'definitely' 'disappointed'\n",
            " 'every' 'exceeding' 'expectations' 'fair' 'for' 'is' 'it' 'love' 'money'\n",
            " 'my' 'not' 'of' 'okay' 'outstanding' 'poor' 'pricing' 'product' 'quality'\n",
            " 'seems' 'sure' 'the' 'this' 'time' 'transparent' 'value']\n",
            "[[0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            " [0 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0]]\n",
            "\n",
            "TF-IDF features:\n",
            "['about' 'amazing' 'and' 'consistently' 'definitely' 'disappointed'\n",
            " 'every' 'exceeding' 'expectations' 'fair' 'for' 'is' 'it' 'love' 'money'\n",
            " 'my' 'not' 'of' 'okay' 'outstanding' 'poor' 'pricing' 'product' 'quality'\n",
            " 'seems' 'sure' 'the' 'this' 'time' 'transparent' 'value']\n",
            "[[0.         0.         0.         0.33451305 0.         0.\n",
            "  0.33451305 0.33451305 0.33451305 0.         0.         0.18845882\n",
            "  0.         0.         0.         0.33451305 0.         0.\n",
            "  0.         0.33451305 0.         0.         0.18845882 0.26988302\n",
            "  0.         0.         0.26988302 0.         0.33451305 0.\n",
            "  0.        ]\n",
            " [0.         0.51748706 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.29154318\n",
            "  0.4175053  0.51748706 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.29154318 0.\n",
            "  0.         0.         0.         0.34656711 0.         0.\n",
            "  0.        ]\n",
            " [0.         0.         0.34674181 0.         0.34674181 0.\n",
            "  0.         0.         0.         0.34674181 0.34674181 0.19534829\n",
            "  0.         0.         0.34674181 0.         0.         0.\n",
            "  0.         0.         0.         0.34674181 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.34674181\n",
            "  0.34674181]\n",
            " [0.         0.         0.         0.         0.         0.43092459\n",
            "  0.         0.         0.         0.         0.         0.24277539\n",
            "  0.         0.         0.         0.         0.         0.43092459\n",
            "  0.         0.         0.43092459 0.         0.24277539 0.34766724\n",
            "  0.         0.         0.34766724 0.28859522 0.         0.\n",
            "  0.        ]\n",
            " [0.394766   0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.31849473 0.         0.         0.         0.394766   0.\n",
            "  0.394766   0.         0.         0.         0.22240428 0.\n",
            "  0.394766   0.394766   0.         0.26437939 0.         0.\n",
            "  0.        ]]\n",
            "\n",
            "N-grams features:\n",
            "[['The product', 'product quality', 'quality is', 'is consistently', 'consistently outstanding', 'outstanding ,', ', exceeding', 'exceeding my', 'my expectations', 'expectations every', 'every time', 'time .'], ['This product', 'product is', 'is amazing', 'amazing !', '! I', 'I love', 'love it', 'it .'], ['Pricing is', 'is fair', 'fair and', 'and transparent', 'transparent definitely', 'definitely value', 'value for', 'for money', 'money .'], ['The quality', 'quality of', 'of this', 'this product', 'product is', 'is poor', 'poor .', '. I', \"I 'm\", \"'m disappointed\", 'disappointed .'], [\"I 'm\", \"'m not\", 'not sure', 'sure about', 'about this', 'this product', 'product .', '. It', 'It seems', 'seems okay', 'okay .']]\n",
            "\n",
            "Part-of-Speech (POS) features:\n",
            "[['DT', 'NN', 'NN', 'VBZ', 'RB', 'JJ', ',', 'VBG', 'PRP$', 'NNS', 'DT', 'NN', '.'], ['DT', 'NN', 'VBZ', 'JJ', '.', 'PRP', 'VBP', 'PRP', '.'], ['NN', 'VBZ', 'JJ', 'CC', 'JJ', 'RB', 'NN', 'IN', 'NN', '.'], ['DT', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'JJ', '.', 'PRP', 'VBP', 'JJ', '.'], ['PRP', 'VBP', 'RB', 'JJ', 'IN', 'DT', 'NN', '.', 'PRP', 'VBZ', 'JJ', '.']]\n",
            "\n",
            "Word Embeddings features:\n",
            "[[[0.4, -0.3, 0.7]], [[0.4, -0.3, 0.7], [0.8, 0.2, -0.4], [0.6, 0.1, 0.5]], [], [[0.4, -0.3, 0.7], [-0.7, -0.5, 0.3], [-0.9, -0.8, 0.2]], [[0.4, -0.3, 0.7], [0.2, 0.3, -0.6]]]\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from nltk.util import ngrams\n",
        "from nltk import pos_tag, word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Sample text data\n",
        "texts = [\n",
        "    \"The product quality is consistently outstanding, exceeding my expectations every time.\",\n",
        "    \"This product is amazing! I love it.\",\n",
        "    \"Pricing is fair and transparent definitely value for money.\",\n",
        "    \"The quality of this product is poor. I'm disappointed.\",\n",
        "    \"I'm not sure about this product. It seems okay.\",\n",
        "]\n",
        "\n",
        "# Bag-of-Words (BoW)\n",
        "vectorizer_bow = CountVectorizer()\n",
        "X_bow = vectorizer_bow.fit_transform(texts)\n",
        "print(\"\\nBag-of-Words (BoW) features:\")\n",
        "print(vectorizer_bow.get_feature_names_out())\n",
        "print(X_bow.toarray())\n",
        "\n",
        "# TF-IDF\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "X_tfidf = vectorizer_tfidf.fit_transform(texts)\n",
        "print(\"\\nTF-IDF features:\")\n",
        "print(vectorizer_tfidf.get_feature_names_out())\n",
        "print(X_tfidf.toarray())\n",
        "\n",
        "# N-grams\n",
        "ngram_range = (1, 2)  # Considering unigrams and bigrams\n",
        "X_ngrams = []\n",
        "for text in texts:\n",
        "    tokens = word_tokenize(text)\n",
        "    ngrams_list = [' '.join(gram) for gram in ngrams(tokens, n=2)]\n",
        "    X_ngrams.append(ngrams_list)\n",
        "print(\"\\nN-grams features:\")\n",
        "print(X_ngrams)\n",
        "\n",
        "# Part-of-Speech (POS) features\n",
        "X_pos = []\n",
        "for text in texts:\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = [tag for word, tag in pos_tag(tokens)]\n",
        "    X_pos.append(pos_tags)\n",
        "print(\"\\nPart-of-Speech (POS) features:\")\n",
        "print(X_pos)\n",
        "\n",
        "# Word Embeddings (For demonstration purposes, let's assume we have pre-trained word embeddings)\n",
        "word_embeddings = {\n",
        "    \"amazing\": [0.8, 0.2, -0.4],\n",
        "    \"love\": [0.6, 0.1, 0.5],\n",
        "    \"poor\": [-0.7, -0.5, 0.3],\n",
        "    \"disappointed\": [-0.9, -0.8, 0.2],\n",
        "    \"okay\": [0.2, 0.3, -0.6],\n",
        "    \"product\": [0.4, -0.3, 0.7],\n",
        "}\n",
        "X_embeddings = []\n",
        "for text in texts:\n",
        "    tokens = word_tokenize(text)\n",
        "    embeddings = [word_embeddings[token] for token in tokens if token in word_embeddings]\n",
        "    X_embeddings.append(embeddings)\n",
        "print(\"\\nWord Embeddings features:\")\n",
        "print(X_embeddings)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cb02ac9-dacd-4705-cbd1-bd07aa469861"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Feature  Rank\n",
            "0             it    30\n",
            "1        product    29\n",
            "2            the    28\n",
            "3        quality    27\n",
            "4             is    26\n",
            "5           time    25\n",
            "6          money    24\n",
            "7   consistently    23\n",
            "8          value    22\n",
            "9    outstanding    21\n",
            "10   transparent    20\n",
            "11            my    19\n",
            "12           for    18\n",
            "13  expectations    17\n",
            "14       pricing    16\n",
            "15         every    15\n",
            "16           and    14\n",
            "17     exceeding    13\n",
            "18    definitely    12\n",
            "19          fair    11\n",
            "20       amazing    10\n",
            "21          this     9\n",
            "22          love     8\n",
            "23         about     7\n",
            "24           not     6\n",
            "25          okay     5\n",
            "26         seems     4\n",
            "27  disappointed     3\n",
            "28          sure     2\n",
            "29            of     1\n",
            "30          poor     1\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Sample text data\n",
        "texts = [\n",
        "    \"The product quality is consistently outstanding, exceeding my expectations every time.\",\n",
        "    \"This product is amazing! I love it.\",\n",
        "    \"Pricing is fair and transparent definitely value for money.\",\n",
        "    \"The quality of this product is poor. I'm disappointed.\",\n",
        "    \"I'm not sure about this product. It seems okay.\",\n",
        "]\n",
        "\n",
        "# Labels\n",
        "labels = [1, 1, 1, 0, 0]  # 1 for positive, 0 for negative\n",
        "\n",
        "# Feature extraction using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X = tfidf_vectorizer.fit_transform(texts)\n",
        "\n",
        "# Feature selection using RFE (using \"Wrapper\" feature selection model from the paper mentioned above)\n",
        "logreg = LogisticRegression()\n",
        "rfe = RFE(estimator=logreg, n_features_to_select=2)\n",
        "rfe.fit(X, labels)\n",
        "\n",
        "# Ranking features based on their importance\n",
        "feature_ranks = sorted(zip(tfidf_vectorizer.get_feature_names_out(), rfe.ranking_), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(feature_ranks, columns=['Feature', 'Rank'])\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xn86dSAH7Ysg",
        "outputId": "f4f32813-9d45-45e9-80a1-b4dced317c82"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.5.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.38.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "087c672a-6adf-4d7d-f926-e44e1f315d9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "   Similarity                                               Text\n",
            "0    0.979436                This product is amazing! I love it.\n",
            "1    0.706400  The product quality is consistently outstandin...\n",
            "2    0.663080  Pricing is fair and transparent definitely val...\n",
            "3    0.539053    I'm not sure about this product. It seems okay.\n",
            "4    0.290109  The quality of this product is poor. I'm disap...\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# Sample query and texts\n",
        "query = \"I loved the product, it was fantastic!\"\n",
        "\n",
        "# Converting the query and texts to BERT embeddings\n",
        "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
        "query_embedding = model.encode(query)\n",
        "text_embeddings = model.encode(texts)\n",
        "\n",
        "# Calculate cosine similarity\n",
        "similarities = cosine_similarity(query_embedding.reshape(1, -1), text_embeddings)\n",
        "\n",
        "# Rank texts based on similarity\n",
        "ranked_texts = sorted(zip(similarities[0], texts), reverse=True)\n",
        "\n",
        "print(\"\\n\")\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(ranked_texts, columns=['Similarity', 'Text'])\n",
        "print(df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "In class I was not sure about these features during the explanation, but after doing this assignment, it feels like I'm confident about feature extraction now.\n",
        "All the features that I worked on were so good in sentiment analysis, we were able to how many reviwes were positive and how many are negative.\n",
        "By that we can say the product is worth buying.\n",
        "\n",
        "I didn't faced any issues in this assignement, the questions were straight forward and answerable.\n",
        "\n",
        "This exercise is related to NLP in several ways. Feature extraction, Text classification, Semantic analysis all these were helpful in real world situations\n",
        "to improve business. All these work will be done using the text data which need to be collected by data professionals, analysed using ML models and to give insights.\n",
        "Which means it is directly related to feild of study data science.\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "c2a5103d-9fd5-427d-dad9-8ccc50cf12f9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nIn class I was not sure about these features during the explanation, but after doing this assignment, it feels like I'm confident about feature extraction now.\\nAll the features that I worked on were so good in sentiment analysis, we were able to how many reviwes were positive and how many are negative.\\nBy that we can say the product is worth buying.\\n\\nI didn't faced any issues in this assignement, the questions were straight forward and answerable.\\n\\nThis exercise is related to NLP in several ways. Feature extraction, Text classification, Semantic analysis all these were helpful in real world situations\\nto improve business. All these work will be done using the text data which need to be collected by data professionals, analysed using ML models and to give insights.\\nWhich means it is directly related to feild of study data science. \\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}