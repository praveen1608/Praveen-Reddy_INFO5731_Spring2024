{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/praveen1608/Praveen-Reddy_INFO5731_Spring2024/blob/main/Kadasani_PraveenReddy_Exercise_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 5**\n",
        "\n",
        "**This exercise aims to provide a comprehensive learning experience in text analysis and machine learning techniques, focusing on both text classification and clustering tasks.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ],
      "metadata": {
        "id": "TU-pLW33lpcS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## **Question 1 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text classification** as well as the performance evaluation. In addition, you are requried to conduct **10 fold cross validation** (https://scikit-learn.org/stable/modules/cross_validation.html) in the training.\n",
        "\n",
        "\n",
        "\n",
        "The dataset can be download from canvas. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algorithms:**\n",
        "\n",
        "*   MultinominalNB\n",
        "*   SVM\n",
        "*   KNN\n",
        "*   Decision tree\n",
        "*   Random Forest\n",
        "*   XGBoost\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "**Evaluation measurement:**\n",
        "\n",
        "\n",
        "*   Accuracy\n",
        "*   Recall\n",
        "*   Precison\n",
        "*   F-1 score\n"
      ],
      "metadata": {
        "id": "loi8Sh7UE6ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection\n",
        "from sklearn import naive_bayes, svm\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def read_data(file_path):\n",
        "    text_data, sentiments = [], []\n",
        "    file_data = open(file_path).read()\n",
        "    for i, j in enumerate(file_data.split(\"\\n\")):\n",
        "        after_split = j.split(' ')\n",
        "        text_data.append(\" \".join(after_split[1:]))\n",
        "        sentiments.append(after_split[0])\n",
        "    return text_data, sentiments\n",
        "\n",
        "\n",
        "def preprocess_data(data_frame):\n",
        "    # Convert sentiment classes to integers\n",
        "    data_frame['Sentimental Value'] = pd.to_numeric(data_frame['Sentimental Value'], errors='coerce')\n",
        "\n",
        "    # Drop rows with NaN values in the target variable\n",
        "    data_frame = data_frame.dropna(subset=['Sentimental Value'])\n",
        "\n",
        "    # Pre-processing\n",
        "    # removal of special characters\n",
        "    data_frame['After noise removal'] = data_frame['Raw Data'].apply(lambda x: ''.join(re.sub(r\"[^a-zA-Z0-9]+\", ' ', char) for char in x))\n",
        "\n",
        "    # removal of Punctuation\n",
        "    data_frame['Punctuation removal'] = data_frame['After noise removal'].str.replace('[^\\w\\s]', '')\n",
        "\n",
        "    # Stopwords removal\n",
        "    stop_word = stopwords.words('english')\n",
        "    data_frame['Stopwords removal'] = data_frame['Punctuation removal'].apply(\n",
        "        lambda x: \" \".join(word for word in x.split() if word not in stop_word))\n",
        "\n",
        "    # Lower Casing\n",
        "    data_frame['Lower casing'] = data_frame['Stopwords removal'].apply(lambda x: \" \".join(word.lower() for word in x.split()))\n",
        "\n",
        "    return data_frame\n",
        "\n",
        "\n",
        "def vectorize_data(train_df, test_df):\n",
        "    # TF-IDF Vectorization\n",
        "    tfidf_vector = TfidfVectorizer(analyzer='word')\n",
        "    tfidf_vector.fit(train_df['Lower casing'])\n",
        "    x_train = tfidf_vector.transform(train_df['Lower casing'])\n",
        "    tfidf_vector_test = TfidfVectorizer(analyzer='word', vocabulary=tfidf_vector.vocabulary_)\n",
        "    tfidf_vector_test.fit(test_df['Lower casing'])\n",
        "    x_test = tfidf_vector_test.transform(test_df['Lower casing'])\n",
        "\n",
        "    return x_train, x_test\n",
        "\n",
        "\n",
        "def evaluate_classifier(classifier, x_train, y_train, x_test, y_test):\n",
        "    classifier.fit(x_train, y_train)\n",
        "    predicted = classifier.predict(x_test)\n",
        "    accuracy = accuracy_score(predicted, y_test)\n",
        "    print(f\"Accuracy of Training data ({classifier.__class__.__name__}): {accuracy}\")\n",
        "\n",
        "    predicted_testing = classifier.predict(x_test)\n",
        "    accuracy_testing = accuracy_score(predicted_testing, y_test)\n",
        "    print(f\"Accuracy of Testing data ({classifier.__class__.__name__}): {accuracy_testing}\")\n",
        "\n",
        "    if 'XGB' not in str(classifier):\n",
        "        scoring = 'accuracy'\n",
        "        kfold = KFold(10, random_state=7, shuffle=True)\n",
        "        cross_val = cross_val_score(classifier, x_test, y_test, cv=kfold, scoring=scoring).mean()\n",
        "        print(f\"Cross-validation score ({classifier.__class__.__name__}): {cross_val}\")\n",
        "\n",
        "    print(f\"\\nClassification Report ({classifier.__class__.__name__}):\\n\")\n",
        "    print(classification_report(y_test, predicted))\n",
        "\n",
        "\n",
        "# Read data\n",
        "training_text_data, training_sentiments = read_data('stsa-train.txt')\n",
        "testing_text_data, testing_sentiments = read_data('stsa-test.txt')\n",
        "\n",
        "# Create data frames\n",
        "training_df = pd.DataFrame(list(zip(training_sentiments, training_text_data)), columns=['Sentimental Value', 'Raw Data'])\n",
        "testing_df = pd.DataFrame(list(zip(testing_sentiments, testing_text_data)), columns=['Sentimental Value', 'Raw Data'])\n",
        "\n",
        "# Pre-process data\n",
        "training_df = preprocess_data(training_df)\n",
        "testing_df = preprocess_data(testing_df)\n",
        "\n",
        "# Vectorize data\n",
        "x_train, x_test = vectorize_data(training_df, testing_df)\n",
        "\n",
        "# Split data for training and testing\n",
        "x_train, x_valid, y_train, y_valid = model_selection.train_test_split(x_train, training_df['Sentimental Value'],\n",
        "                                                                      test_size=0.2, random_state=42)\n",
        "\n",
        "# Ensure target variables are integers\n",
        "y_train = y_train.astype(int)\n",
        "y_valid = y_valid.astype(int)\n",
        "\n",
        "# List of classifiers\n",
        "classifiers = [\n",
        "    naive_bayes.MultinomialNB(),\n",
        "    svm.SVC(),\n",
        "    KNeighborsClassifier(n_neighbors=5),\n",
        "    DecisionTreeClassifier(),\n",
        "    RandomForestClassifier(),\n",
        "    XGBClassifier()\n",
        "]\n",
        "\n",
        "# Evaluate each classifier\n",
        "for classifier in classifiers:\n",
        "    print(f\"Evaluating {classifier.__class__.__name__}:\")\n",
        "    evaluate_classifier(classifier, x_train, y_train, x_valid, y_valid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOlqc_y99xyy",
        "outputId": "b9412508-1821-420e-8e6c-4aefef293ef9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating MultinomialNB:\n",
            "Accuracy of Training data (MultinomialNB): 0.7911849710982659\n",
            "Accuracy of Testing data (MultinomialNB): 0.7911849710982659\n",
            "Cross-validation score (MultinomialNB): 0.7015848191012408\n",
            "\n",
            "Classification Report (MultinomialNB):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.69      0.76       671\n",
            "           1       0.75      0.89      0.81       713\n",
            "\n",
            "    accuracy                           0.79      1384\n",
            "   macro avg       0.80      0.79      0.79      1384\n",
            "weighted avg       0.80      0.79      0.79      1384\n",
            "\n",
            "Evaluating SVC:\n",
            "Accuracy of Training data (SVC): 0.796242774566474\n",
            "Accuracy of Testing data (SVC): 0.796242774566474\n",
            "Cross-validation score (SVC): 0.7008758210822645\n",
            "\n",
            "Classification Report (SVC):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.73      0.78       671\n",
            "           1       0.77      0.86      0.81       713\n",
            "\n",
            "    accuracy                           0.80      1384\n",
            "   macro avg       0.80      0.79      0.79      1384\n",
            "weighted avg       0.80      0.80      0.80      1384\n",
            "\n",
            "Evaluating KNeighborsClassifier:\n",
            "Accuracy of Training data (KNeighborsClassifier): 0.523121387283237\n",
            "Accuracy of Testing data (KNeighborsClassifier): 0.523121387283237\n",
            "Cross-validation score (KNeighborsClassifier): 0.6062350119904077\n",
            "\n",
            "Classification Report (KNeighborsClassifier):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.52      0.51       671\n",
            "           1       0.54      0.53      0.53       713\n",
            "\n",
            "    accuracy                           0.52      1384\n",
            "   macro avg       0.52      0.52      0.52      1384\n",
            "weighted avg       0.52      0.52      0.52      1384\n",
            "\n",
            "Evaluating DecisionTreeClassifier:\n",
            "Accuracy of Training data (DecisionTreeClassifier): 0.6777456647398844\n",
            "Accuracy of Testing data (DecisionTreeClassifier): 0.6777456647398844\n",
            "Cross-validation score (DecisionTreeClassifier): 0.611364821186529\n",
            "\n",
            "Classification Report (DecisionTreeClassifier):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.68      0.67       671\n",
            "           1       0.69      0.67      0.68       713\n",
            "\n",
            "    accuracy                           0.68      1384\n",
            "   macro avg       0.68      0.68      0.68      1384\n",
            "weighted avg       0.68      0.68      0.68      1384\n",
            "\n",
            "Evaluating RandomForestClassifier:\n",
            "Accuracy of Training data (RandomForestClassifier): 0.736271676300578\n",
            "Accuracy of Testing data (RandomForestClassifier): 0.736271676300578\n",
            "Cross-validation score (RandomForestClassifier): 0.6473725367532062\n",
            "\n",
            "Classification Report (RandomForestClassifier):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.66      0.71       671\n",
            "           1       0.72      0.81      0.76       713\n",
            "\n",
            "    accuracy                           0.74      1384\n",
            "   macro avg       0.74      0.73      0.73      1384\n",
            "weighted avg       0.74      0.74      0.73      1384\n",
            "\n",
            "Evaluating XGBClassifier:\n",
            "Accuracy of Training data (XGBClassifier): 0.6986994219653179\n",
            "Accuracy of Testing data (XGBClassifier): 0.6986994219653179\n",
            "\n",
            "Classification Report (XGBClassifier):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.53      0.63       671\n",
            "           1       0.66      0.86      0.75       713\n",
            "\n",
            "    accuracy                           0.70      1384\n",
            "   macro avg       0.72      0.69      0.69      1384\n",
            "weighted avg       0.72      0.70      0.69      1384\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## **Question 2 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text clustering**.\n",
        "\n",
        "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
        "(You can also use different text data which you want)\n",
        "\n",
        "**Apply the listed clustering methods to the dataset:**\n",
        "*   K-means\n",
        "*   DBSCAN\n",
        "*   Hierarchical clustering\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "You can refer to of the codes from  the follwing link below.\n",
        "https://www.kaggle.com/karthik3890/text-clustering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv('/content/Amazon_Unlocked_Mobile.csv')\n",
        "\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "NljznFrHTHsX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fabe8ae9-d235-4260-9ad0-033861778a18"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        Product Name Brand Name   Price  \\\n",
            "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "\n",
            "   Rating                                            Reviews  Review Votes  \n",
            "0       5  I feel so LUCKY to have found this used (phone...           1.0  \n",
            "1       4  nice phone, nice up grade from my pantach revu...           0.0  \n",
            "2       5                                       Very pleased           0.0  \n",
            "3       4  It works good but it goes slow sometimes but i...           0.0  \n",
            "4       4  Great phone to replace my lost phone. The only...           0.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install data_cleaning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KjqT4KqZqgO",
        "outputId": "f69f0c0c-2844-43fc-9420-0fb2242965e3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting data_cleaning\n",
            "  Downloading data_cleaning-1.0.1-py3-none-any.whl (6.3 kB)\n",
            "Collecting missingpy (from data_cleaning)\n",
            "  Downloading missingpy-0.2.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m759.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from data_cleaning) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from data_cleaning) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from data_cleaning) (2.0.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from data_cleaning) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from data_cleaning) (2023.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from data_cleaning) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from data_cleaning) (1.11.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from data_cleaning) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.10/dist-packages (from data_cleaning) (3.4.0)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->data_cleaning) (2024.1)\n",
            "Installing collected packages: missingpy, data_cleaning\n",
            "Successfully installed data_cleaning-1.0.1 missingpy-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF VECTORIZATION\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vect = TfidfVectorizer()\n",
        "tfidf_vects = tfidf_vect.fit_transform(df['Reviews'].values.astype('U'))\n",
        "names= tfidf_vect.get_feature_names()\n"
      ],
      "metadata": {
        "id": "-SE1QlTGT-59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ELBOW METHOD\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "wcss = []\n",
        "for i in range(2,12):\n",
        "    kmeans = KMeans(n_clusters = i, init = \"k-means++\", random_state = 101)\n",
        "    kmeans.fit(tfidf_vects)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize = (11,6))\n",
        "plt.plot(range(2,12), wcss, marker = \"o\")\n",
        "plt.title (\"The Elbow Method\")\n",
        "plt.xlabel(\"Number of clusters\")\n",
        "plt.ylabel(\"WCSS\")"
      ],
      "metadata": {
        "id": "fs3QUaZGT-8-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "de6bfb92-42cd-4b91-b5ef-bb81461f4fb0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tfidf_vects' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d034c55df02e>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"k-means++\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m101\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_vects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mwcss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minertia_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tfidf_vects' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QJU_ZKVRT-_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qjNCZLJuT_CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "exQkE_z0T_FL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9U3W3WXuT_H9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UJ-03U8dT_KT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E5JyoaAoT_Nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In one paragraph, please compare the results of K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT.**"
      ],
      "metadata": {
        "id": "tRijW2aLGONl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write your response here:**\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pIYCj5qyGfSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Through this exercise I got hands on experience with Text classification and clustering. I would say working on this exercise is not easy when compared to previous ones.\n",
        "I have faced few issues while working on BERT and word2Vec in question number 1. It took a while to run the code.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}